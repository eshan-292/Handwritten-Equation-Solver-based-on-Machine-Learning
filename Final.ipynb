{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART1 -> CLASSIFICATION TO PREFIX POSTFIX INFIX(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45000 validated image filenames belonging to 3 classes.\n",
      "Found 5000 validated image filenames belonging to 3 classes.\n",
      "Epoch 1/10\n",
      "450/450 [==============================] - 139s 309ms/step - loss: 0.5118 - accuracy: 0.7440 - val_loss: 34.3288 - val_accuracy: 0.8962\n",
      "Epoch 2/10\n",
      "163/450 [=========>....................] - ETA: 1:13 - loss: 0.2459 - accuracy: 0.8963"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-6de37111f7c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;31m# FITTING THE MODEL WITH TRAINING SET AND PREDICTING ACCURACY ON TEST SET\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#PART 1\n",
    "\n",
    "#IMPORTING THE LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#READING THE DATA AND SPLITTING THE CSV FILE\n",
    "train = pd.read_csv('annotations.csv').iloc[0:45000,:]\n",
    "test = pd.read_csv('annotations.csv').iloc[45000:,:]\n",
    "# IMPORTING LIBRARIES FOR CNN\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "#K.set_image_dim_ordering('th')\n",
    "K.set_image_data_format(\"channels_first\")\n",
    "# TO CONVERT IMAGES TO TENSOR\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   \n",
    "                                   \n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_gen = ImageDataGenerator(rescale = 1.255)\n",
    "train_data = train_datagen.flow_from_dataframe(dataframe = train, \n",
    "directory = 'data/training_set', x_col = 'Image', \n",
    "y_col = 'Label', seed = 42,\n",
    "batch_size = 100, shuffle = True, \n",
    "class_mode=\"categorical\",target_size = (64, 64),color_mode='grayscale')\n",
    "\n",
    "test_data = test_gen.flow_from_dataframe(dataframe = test, \n",
    "directory = 'data/test_set', x_col = 'Image', \n",
    "y_col ='Label',\n",
    "batch_size = 100, shuffle = True, \n",
    "class_mode=\"categorical\",target_size = (64, 64),color_mode='grayscale')\n",
    "\n",
    "# CREATING AN OBJECT OF SEQUENTIAL CLASS\n",
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "# ADDING CONVULUTION LAYERS\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 1]))\n",
    "\n",
    "#POOLING\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "#ADDING MORE LAYERS\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "#FLATTENING \n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "# ADDING OUTPUT LAYER\n",
    "cnn.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n",
    "\n",
    "#COMPILING VARIOUS LAYERS\n",
    "cnn.compile(optimizer = 'adam',loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# FITTING THE MODEL WITH TRAINING SET AND PREDICTING ACCURACY ON TEST SET\n",
    "cnn.fit(x = train_data,validation_data=test_data, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('annotations.csv').iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "results = []\n",
    "test_size = 100\n",
    "value = \"\"\n",
    "correct = 0\n",
    "n = test_size\n",
    "for i in range (1,n+1):\n",
    "        test_image = image.load_img(\"data/test_set/\"+str(i+45000)+\".jpg\", target_size = (64, 64),color_mode = 'grayscale')\n",
    "#        test_image.show()\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        test_image = np.expand_dims(test_image, axis = -1)\n",
    "        #PREDCITING ON FIRST PART\n",
    "        result = cnn.predict(test_image)\n",
    "        if result[0][0]==1:\n",
    "            value = \"infix\"\n",
    "        elif result[0][1]==1:\n",
    "            value = \"postfix\"\n",
    "        else:\n",
    "            value = \"prefix\"\n",
    "        if value==dataset[45000+i-1]:\n",
    "            correct+=1\n",
    "        results.append(value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct/n\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['infix', 'infix', 'prefix', 'infix', 'prefix', 'postfix', 'prefix', 'postfix', 'postfix', 'postfix', 'infix', 'infix', 'infix', 'prefix', 'prefix', 'postfix', 'prefix', 'postfix', 'prefix', 'infix', 'prefix', 'postfix', 'postfix', 'prefix', 'prefix', 'prefix', 'prefix', 'postfix', 'prefix', 'prefix', 'prefix', 'infix', 'postfix', 'postfix', 'postfix', 'prefix', 'prefix', 'postfix', 'infix', 'infix', 'prefix', 'postfix', 'infix', 'postfix', 'infix', 'prefix', 'postfix', 'infix', 'infix', 'prefix', 'prefix', 'prefix', 'infix', 'prefix', 'postfix', 'infix', 'infix', 'postfix', 'prefix', 'infix', 'prefix', 'postfix', 'prefix', 'infix', 'infix', 'prefix', 'postfix', 'postfix', 'prefix', 'postfix', 'postfix', 'infix', 'prefix', 'prefix', 'infix', 'postfix', 'postfix', 'postfix', 'infix', 'prefix', 'infix', 'postfix', 'prefix', 'prefix', 'infix', 'infix', 'postfix', 'infix', 'infix', 'infix', 'postfix', 'postfix', 'postfix', 'infix', 'prefix', 'prefix', 'postfix', 'prefix', 'postfix', 'infix']\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d= np.column_stack((results,dataset[45000:45000+n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "(pd.DataFrame(array_2d)).to_csv(\"results2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING THE MODEL\n",
    "\n",
    "model_json = cnn.to_json()\n",
    "with open(\"model_final_1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "cnn.save_weights(\"model_final_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lOADING THE SAVED MODEL\n",
    "from keras.models import model_from_json\n",
    "json_file = open('model_final_1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_final_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to demonstrate the use of 'sys' module\n",
    "# for command line arguments\n",
    "  \n",
    "import sys\n",
    "  \n",
    "# command line arguments are stored in the form\n",
    "# of list in sys.argv\n",
    "argumentList = sys.argv\n",
    "#print argumentList\n",
    "  \n",
    "# Print the name of file\n",
    "#print sys.argv[0]\n",
    "  \n",
    "# Print the first argument after the name of file\n",
    "testfolder_path=sys.argv[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING LIBRARIES FOR FUTURE USE AND IMAGE MODIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1212)\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "#K.set_image_dim_ordering('th')\n",
    "K.set_image_data_format(\"channels_first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLITTING THE IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANUAL LABELLING OF AROUND 3300 IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATING TRAINING SET FROM SPLITTED IMAGES(RECOGNISING ALL 14 CHARACTERS(10 DIGITS AND 4 OPERATORS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8050 images belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory(directory='NEW/',\n",
    "                                    \n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 50,\n",
    "                                                 class_mode = \"categorical\",color_mode = 'grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING THE VARIOUS LAYERS AND TRAINING THE MODEL ON TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "161/161 [==============================] - 29s 177ms/step - loss: 1.4294 - accuracy: 0.5466 - val_loss: 0.7087 - val_accuracy: 0.7826\n",
      "Epoch 2/50\n",
      "161/161 [==============================] - 28s 177ms/step - loss: 0.5982 - accuracy: 0.8183 - val_loss: 0.4347 - val_accuracy: 0.8673\n",
      "Epoch 3/50\n",
      "161/161 [==============================] - 29s 180ms/step - loss: 0.4094 - accuracy: 0.8724 - val_loss: 0.3130 - val_accuracy: 0.8998\n",
      "Epoch 4/50\n",
      "161/161 [==============================] - 29s 179ms/step - loss: 0.3208 - accuracy: 0.9011 - val_loss: 0.2761 - val_accuracy: 0.9181\n",
      "Epoch 5/50\n",
      "161/161 [==============================] - 30s 189ms/step - loss: 0.2693 - accuracy: 0.9163 - val_loss: 0.2343 - val_accuracy: 0.9272\n",
      "Epoch 6/50\n",
      "161/161 [==============================] - 29s 183ms/step - loss: 0.2273 - accuracy: 0.9320 - val_loss: 0.1872 - val_accuracy: 0.9461\n",
      "Epoch 7/50\n",
      "161/161 [==============================] - 30s 185ms/step - loss: 0.1947 - accuracy: 0.9374 - val_loss: 0.1635 - val_accuracy: 0.9491\n",
      "Epoch 8/50\n",
      "161/161 [==============================] - 28s 175ms/step - loss: 0.1701 - accuracy: 0.9484 - val_loss: 0.1402 - val_accuracy: 0.9584\n",
      "Epoch 9/50\n",
      "161/161 [==============================] - 28s 175ms/step - loss: 0.1530 - accuracy: 0.9516 - val_loss: 0.1170 - val_accuracy: 0.9656\n",
      "Epoch 10/50\n",
      "161/161 [==============================] - 29s 183ms/step - loss: 0.1226 - accuracy: 0.9630 - val_loss: 0.0963 - val_accuracy: 0.9707\n",
      "Epoch 11/50\n",
      "161/161 [==============================] - 28s 177ms/step - loss: 0.1187 - accuracy: 0.9636 - val_loss: 0.0893 - val_accuracy: 0.9765\n",
      "Epoch 12/50\n",
      "161/161 [==============================] - 29s 179ms/step - loss: 0.1106 - accuracy: 0.9657 - val_loss: 0.0960 - val_accuracy: 0.9699\n",
      "Epoch 13/50\n",
      "161/161 [==============================] - 31s 195ms/step - loss: 0.1021 - accuracy: 0.9684 - val_loss: 0.0887 - val_accuracy: 0.9730\n",
      "Epoch 14/50\n",
      "161/161 [==============================] - 31s 194ms/step - loss: 0.0854 - accuracy: 0.9723 - val_loss: 0.0809 - val_accuracy: 0.9760\n",
      "Epoch 15/50\n",
      "161/161 [==============================] - 34s 212ms/step - loss: 0.0878 - accuracy: 0.9727 - val_loss: 0.0714 - val_accuracy: 0.9793\n",
      "Epoch 16/50\n",
      "161/161 [==============================] - 34s 214ms/step - loss: 0.0864 - accuracy: 0.9745 - val_loss: 0.0692 - val_accuracy: 0.9769\n",
      "Epoch 17/50\n",
      "161/161 [==============================] - 31s 194ms/step - loss: 0.0759 - accuracy: 0.9774 - val_loss: 0.0696 - val_accuracy: 0.9791\n",
      "Epoch 18/50\n",
      "161/161 [==============================] - 34s 209ms/step - loss: 0.0718 - accuracy: 0.9765 - val_loss: 0.0690 - val_accuracy: 0.9775\n",
      "Epoch 19/50\n",
      "161/161 [==============================] - 31s 191ms/step - loss: 0.0640 - accuracy: 0.9791 - val_loss: 0.0584 - val_accuracy: 0.9821\n",
      "Epoch 20/50\n",
      "161/161 [==============================] - 31s 195ms/step - loss: 0.0564 - accuracy: 0.9826 - val_loss: 0.0575 - val_accuracy: 0.9829\n",
      "Epoch 21/50\n",
      "161/161 [==============================] - 29s 179ms/step - loss: 0.0571 - accuracy: 0.9830 - val_loss: 0.0528 - val_accuracy: 0.9839\n",
      "Epoch 22/50\n",
      "161/161 [==============================] - 30s 183ms/step - loss: 0.0503 - accuracy: 0.9846 - val_loss: 0.0439 - val_accuracy: 0.9855\n",
      "Epoch 23/50\n",
      "161/161 [==============================] - 31s 195ms/step - loss: 0.0637 - accuracy: 0.9815 - val_loss: 0.0773 - val_accuracy: 0.9719\n",
      "Epoch 24/50\n",
      "161/161 [==============================] - 30s 186ms/step - loss: 0.0570 - accuracy: 0.9840 - val_loss: 0.0472 - val_accuracy: 0.9845\n",
      "Epoch 25/50\n",
      "161/161 [==============================] - 29s 181ms/step - loss: 0.0564 - accuracy: 0.9832 - val_loss: 0.0500 - val_accuracy: 0.9855\n",
      "Epoch 26/50\n",
      "161/161 [==============================] - 29s 178ms/step - loss: 0.0465 - accuracy: 0.9870 - val_loss: 0.0717 - val_accuracy: 0.9759\n",
      "Epoch 27/50\n",
      "161/161 [==============================] - 29s 180ms/step - loss: 0.0426 - accuracy: 0.9863 - val_loss: 0.0332 - val_accuracy: 0.9898\n",
      "Epoch 28/50\n",
      "161/161 [==============================] - 29s 181ms/step - loss: 0.0435 - accuracy: 0.9866 - val_loss: 0.0418 - val_accuracy: 0.9855\n",
      "Epoch 29/50\n",
      "161/161 [==============================] - 29s 183ms/step - loss: 0.0432 - accuracy: 0.9873 - val_loss: 0.0584 - val_accuracy: 0.9810\n",
      "Epoch 30/50\n",
      "161/161 [==============================] - 30s 184ms/step - loss: 0.0430 - accuracy: 0.9872 - val_loss: 0.0470 - val_accuracy: 0.9852\n",
      "Epoch 31/50\n",
      "161/161 [==============================] - 29s 182ms/step - loss: 0.0453 - accuracy: 0.9858 - val_loss: 0.0436 - val_accuracy: 0.9857\n",
      "Epoch 32/50\n",
      "161/161 [==============================] - 29s 180ms/step - loss: 0.0427 - accuracy: 0.9868 - val_loss: 0.0393 - val_accuracy: 0.9875\n",
      "Epoch 33/50\n",
      "161/161 [==============================] - 30s 185ms/step - loss: 0.0465 - accuracy: 0.9867 - val_loss: 0.0402 - val_accuracy: 0.9884\n",
      "Epoch 34/50\n",
      "161/161 [==============================] - 29s 179ms/step - loss: 0.0361 - accuracy: 0.9896 - val_loss: 0.0436 - val_accuracy: 0.9853\n",
      "Epoch 35/50\n",
      "161/161 [==============================] - 29s 181ms/step - loss: 0.0354 - accuracy: 0.9901 - val_loss: 0.0491 - val_accuracy: 0.9853\n",
      "Epoch 36/50\n",
      "161/161 [==============================] - 30s 185ms/step - loss: 0.0310 - accuracy: 0.9897 - val_loss: 0.0226 - val_accuracy: 0.9934\n",
      "Epoch 37/50\n",
      "161/161 [==============================] - 31s 190ms/step - loss: 0.0344 - accuracy: 0.9906 - val_loss: 0.0249 - val_accuracy: 0.9932\n",
      "Epoch 38/50\n",
      "161/161 [==============================] - 29s 181ms/step - loss: 0.0267 - accuracy: 0.9918 - val_loss: 0.0243 - val_accuracy: 0.9933\n",
      "Epoch 39/50\n",
      "161/161 [==============================] - 31s 194ms/step - loss: 0.0298 - accuracy: 0.9913 - val_loss: 0.0264 - val_accuracy: 0.9909\n",
      "Epoch 40/50\n",
      "161/161 [==============================] - 30s 184ms/step - loss: 0.0370 - accuracy: 0.9884 - val_loss: 0.0226 - val_accuracy: 0.9932\n",
      "Epoch 41/50\n",
      "161/161 [==============================] - 29s 179ms/step - loss: 0.0234 - accuracy: 0.9934 - val_loss: 0.0381 - val_accuracy: 0.9868\n",
      "Epoch 42/50\n",
      "161/161 [==============================] - 30s 184ms/step - loss: 0.0298 - accuracy: 0.9909 - val_loss: 0.0259 - val_accuracy: 0.9916\n",
      "Epoch 43/50\n",
      "161/161 [==============================] - 29s 179ms/step - loss: 0.0305 - accuracy: 0.9911 - val_loss: 0.0328 - val_accuracy: 0.9893\n",
      "Epoch 44/50\n",
      "161/161 [==============================] - 29s 179ms/step - loss: 0.0329 - accuracy: 0.9893 - val_loss: 0.0250 - val_accuracy: 0.9922\n",
      "Epoch 45/50\n",
      "161/161 [==============================] - 30s 186ms/step - loss: 0.0308 - accuracy: 0.9913 - val_loss: 0.0193 - val_accuracy: 0.9947\n",
      "Epoch 46/50\n",
      "161/161 [==============================] - 30s 184ms/step - loss: 0.0195 - accuracy: 0.9937 - val_loss: 0.0273 - val_accuracy: 0.9924\n",
      "Epoch 47/50\n",
      "161/161 [==============================] - 29s 180ms/step - loss: 0.0345 - accuracy: 0.9892 - val_loss: 0.0716 - val_accuracy: 0.9784\n",
      "Epoch 48/50\n",
      "161/161 [==============================] - 30s 189ms/step - loss: 0.0402 - accuracy: 0.9868 - val_loss: 0.0227 - val_accuracy: 0.9920\n",
      "Epoch 49/50\n",
      "161/161 [==============================] - 28s 175ms/step - loss: 0.0236 - accuracy: 0.9929 - val_loss: 0.0184 - val_accuracy: 0.9947\n",
      "Epoch 50/50\n",
      "161/161 [==============================] - 28s 174ms/step - loss: 0.0236 - accuracy: 0.9922 - val_loss: 0.0265 - val_accuracy: 0.9909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c5517358e0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn2 = tf.keras.models.Sequential()\n",
    "\n",
    "cnn2.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 1]))\n",
    "\n",
    "cnn2.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "cnn2.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn2.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "cnn2.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn2.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "cnn2.add(tf.keras.layers.Flatten())\n",
    "\n",
    "cnn2.add(tf.keras.layers.Dense(units=200, activation='relu'))\n",
    "\n",
    "cnn2.add(tf.keras.layers.Dense(units=14, activation='softmax'))\n",
    "\n",
    "cnn2.compile(optimizer = 'adam',loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "cnn2.fit(x = training_set,validation_data=training_set, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABELS OF VARIUOS DIGITS AND OPERATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+': 0,\n",
       " '-': 1,\n",
       " '0': 2,\n",
       " '1': 3,\n",
       " '2': 4,\n",
       " '3': 5,\n",
       " '4': 6,\n",
       " '5': 7,\n",
       " '6': 8,\n",
       " '7': 9,\n",
       " '8': 10,\n",
       " '9': 11,\n",
       " 'divide': 12,\n",
       " 'multiply': 13}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING THE MODEL ON A SINGLE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('im120.jpg', target_size = (64, 64),color_mode = 'grayscale')\n",
    "test_image.show()\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = -1)\n",
    "result = cnn2.predict(test_image)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = cnn2.to_json()\n",
    "with open(\"model_final_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "cnn2.save_weights(\"model_final_2.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING THE MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lOADING THE SAVED MODEL\n",
    "json_file = open('model_final_2.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "cnn2 = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "cnn2.load_weights(\"model_final_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTING THE MODEL ON SOME OF THE LAST 5000(NON SPLITTED) IMAGES OF THE TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING THE VALUES OF THE DATASET\n",
    "dataset = pd.read_csv('annotations.csv').iloc[:,2].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  0, 21, ...,  1,  2, -2], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to demonstrate the use of 'sys' module\n",
    "# for command line arguments\n",
    "  \n",
    "import sys\n",
    "  \n",
    "# command line arguments are stored in the form\n",
    "# of list in sys.argv\n",
    "argumentList = sys.argv\n",
    "#print argumentList\n",
    "  \n",
    "# Print the name of file\n",
    "#print sys.argv[0]\n",
    "  \n",
    "# Print the first argument after the name of file\n",
    "testfolder_path=sys.argv[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTION TO SPLIT THE IMAGES OF TEST SET AND THEN PREDICT OUTPUT ON EACH INDIVIDUAL IMAGE AND COMBINE TO GENERATE FINAL VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageOps\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "correct=0\n",
    "new_results =[]\n",
    "# GET DIGIT FUNCTION TO GET INDICE CORRESPONDING TO THE DIGIT\n",
    "def get_digit(arr):\n",
    "    for i in range(0,len(arr)):\n",
    "        if arr[i]==1:\n",
    "            return i\n",
    "    return 0\n",
    "\n",
    "# TEST SIZE \n",
    "test_size = 1000\n",
    "n = test_size #should be less than 5000\n",
    "# FOR EVERY IMAGE IN TEST_SET \n",
    "for i in range(1,n+1):\n",
    "# for filename in os.listdir(testfolder_path):\n",
    "#         im = cv2.imread(os.path.join(folder,filename))\n",
    "\n",
    " \n",
    "    # Opens a image in RGB mode\n",
    "        im = Image.open('data/test_set'+\"/\"+str(i+45000)+\".jpg\")\n",
    "        im = ImageOps.grayscale(im)\n",
    "#         im.show()\n",
    "\n",
    "        # Size of the image in pixels (size of original image)\n",
    "        # (This is not mandatory)\n",
    "        width, height = im.size\n",
    "\n",
    "        # Setting the points for cropped image\n",
    "        left = 0\n",
    "        top = 0\n",
    "        right = width/3\n",
    "        bottom = height\n",
    "        # Cropped image of above dimension\n",
    "        # (It will not change original image)\n",
    "        im1 = im.crop((left, top, right, bottom))\n",
    "        im1.save('im1'+str(i)+'.jpg')\n",
    "        test_image = image.load_img('im1'+str(i)+'.jpg', target_size = (64, 64),color_mode = 'grayscale')\n",
    "#         test_image.show()\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        test_image = np.expand_dims(test_image, axis = -1)\n",
    "        #PREDCITING ON FIRST PART\n",
    "        result1 = cnn2.predict(test_image)\n",
    "#         print(result1)\n",
    "\n",
    "        # Shows the image in image viewer\n",
    "\n",
    "\n",
    "\n",
    "        # Setting the points for cropped image\n",
    "        left = width/3\n",
    "        top = 0\n",
    "        right = 2*width/3\n",
    "        bottom = height\n",
    "\n",
    "        # Cropped image of above dimension\n",
    "        im2 = im.crop((left, top, right, bottom))\n",
    "        im2.save('im2'+str(i)+'.jpg')\n",
    "        test_image = image.load_img('im2'+str(i)+'.jpg', target_size = (64, 64),color_mode = 'grayscale')\n",
    "#         test_image.show()\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        test_image = np.expand_dims(test_image, axis = -1)\n",
    "        \n",
    "        \n",
    "        #PREDICTING ON SECOND PART\n",
    "        result2 = cnn2.predict(test_image)\n",
    "\n",
    "        # Shows the image in image viewer\n",
    "\n",
    "\n",
    "        # Setting the points for cropped image\n",
    "        left = 2*width/3\n",
    "        top = 0\n",
    "        right = width\n",
    "        bottom = height\n",
    "\n",
    "        # Cropped image of above dimension\n",
    "        im3 = im.crop((left, top, right, bottom))\n",
    "        im3.save('im3'+str(i)+'.jpg')\n",
    "        test_image = image.load_img('im3'+str(i)+'.jpg', target_size = (64, 64),color_mode = 'grayscale')\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        test_image = np.expand_dims(test_image, axis = -1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #PREDICTING ON THIRD PART\n",
    "        result3 = cnn2.predict(test_image)\n",
    "        \n",
    "        # DECIDING WHICH IS OPERATOR AND WHICH IS OPERAND\n",
    "        if result1[0][0]==1 or result1[0][1]==1 or result1[0][12]==1 or result1[0][13]==1:\n",
    "            operator = result1\n",
    "            op1 = get_digit(result2[0]) # GET INDICES CORRESPONDING TO DIGIT\n",
    "            op2 = get_digit(result3[0]) # ACTUAL DIGIT WILL BE INDICE-\n",
    "            # COMPUTE THE VALUE DEPENDING ON THE OPERATOR \n",
    "            if operator[0][0]==1:       \n",
    "                value=float(op1)-2+float(op2)-2\n",
    "            elif operator[0][1]==1:\n",
    "                value = float(op1)-2-(float(op2)-2)\n",
    "            elif operator[0][12]==1:\n",
    "                if(float(op2)-2==0):value=0\n",
    "                    #TO AVOID DIVISION BY ZERO ERROR\n",
    "                else : value = (float(op1)-2)/(float(op2)-2)\n",
    "            elif operator[0][13]==1:\n",
    "                value = (float(op1)-2)*(float(op2)-2)\n",
    "        elif result2[0][0]==1 or result2[0][1]==1 or result2[0][12]==1 or result2[0][13]==1:\n",
    "            operator = result2\n",
    "            op1 = get_digit(result1[0])\n",
    "            op2 = get_digit(result3[0])\n",
    "            if operator[0][0]==1:\n",
    "                value=float(op1)-2+float(op2)-2\n",
    "            elif operator[0][1]==1:\n",
    "                value = float(op1)-2-(float(op2)-2)\n",
    "            elif operator[0][12]==1:\n",
    "                if(float(op2)-2==0):value=0\n",
    "                else : value = (float(op1)-2)/(float(op2)-2)\n",
    "            elif operator[0][13]==1:\n",
    "                value = (float(op1)-2)*(float(op2)-2)\n",
    "        else :\n",
    "            operator = result3\n",
    "            op1 = get_digit(result1[0])\n",
    "            op2 = get_digit(result2[0])\n",
    "            if operator[0][0]==1:\n",
    "                value=float(op1)-2+float(op2)-2\n",
    "            elif operator[0][1]==1:\n",
    "                value = float(op1)-2-(float(op2)-2)\n",
    "            elif operator[0][12]==1:\n",
    "                if(float(op2)-2==0):value=0\n",
    "                else : value = (float(op1)-2)/(float(op2)-2)\n",
    "            elif operator[0][13]==1:\n",
    "                value = (float(op1)-2)*(float(op2)-2)\n",
    "        \n",
    "        # adding the value to list of results \n",
    "        new_results.append(value)\n",
    "         \n",
    "        # counting the no of correct predictions \n",
    "        if dataset[45000+i-1]==value:\n",
    "            correct+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the results(predicted value) with the actual value in 2d array \n",
    "array_2d= np.column_stack((new_results,dataset[45000:45000+n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.]\n",
      " [ 4.  4.]\n",
      " [ 6.  6.]\n",
      " ...\n",
      " [18. 18.]\n",
      " [10. 10.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(array_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.887\n"
     ]
    }
   ],
   "source": [
    "#predicting the accuracy\n",
    "accuracy = correct/(n)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1c556bdd790>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot y=x (orange) with the points corresponing to prediction\n",
    "plt.scatter(array_2d[:,0],array_2d[:,1])\n",
    "plt.scatter(array_2d[:,1],array_2d[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting 2d array to dataframe and then converting dataframe to csv\n",
    "import pandas as pd\n",
    "(pd.DataFrame(array_2d)).to_csv(\"Team_Name_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
